{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3483ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/zjwu/anaconda3/envs/cuda11/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052bf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./stock_dataset/Combined_train.csv\")\n",
    "df_valid = pd.read_csv(\"./stock_dataset/Combined_valid.csv\")\n",
    "df_test = pd.read_csv(\"./stock_dataset/Combined_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef1ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "DistilBertModel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1331c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df, tokenizer):\n",
    "    headline=[]\n",
    "    for row in range(0,len(df.index)):\n",
    "        headline.append(\" \".join(str(x) for x in df.iloc[row,2:27]))\n",
    "        \n",
    "    clean_headline=[]\n",
    "    for i in range(0,len(headline)):\n",
    "        clean_headline.append(re.sub(\"b[(')]\",'',headline[i])) #remove b'\n",
    "        clean_headline[i]=re.sub('b[(\")]','',clean_headline[i]) #remove b\"\n",
    "        clean_headline[i]=re.sub(\"\\'\",'',clean_headline[i]) #remove \\'\n",
    "        \n",
    "    df['Combined_news'] = clean_headline\n",
    "    df[\"tokenized_news\"] = df[\"Combined_news\"].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=512))\n",
    "    \n",
    "    return df[['Date', 'Label', 'tokenized_news']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34306f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(data, row_idx, m, n):\n",
    "    m_days_news = []\n",
    "    n_days_news = []\n",
    "\n",
    "    for i in range(max(m, n)):\n",
    "        if row_idx - i >= 0:\n",
    "            daily_news = data.iloc[row_idx - i]\n",
    "            tokenized_news = daily_news[\"tokenized_news\"]\n",
    "            tokenized_news_tensor = torch.tensor(tokenized_news['input_ids']).unsqueeze(0)\n",
    "            if i < m:\n",
    "                \n",
    "                m_days_news.append(tokenized_news_tensor)\n",
    "            if i < n:\n",
    "                n_days_news.append(tokenized_news_tensor)\n",
    "\n",
    "    return m_days_news, n_days_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae000c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = combine_text(df_train, tokenizer)\n",
    "df_valid = combine_text(df_valid, tokenizer)\n",
    "df_test = combine_text(df_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c5018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ae1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5  # Number of past days for GRU model\n",
    "N = 7  # Number of past days for Attention model\n",
    "\n",
    "df_train[\"input\"] = [preprocess_news(df_train, idx, M, N) for idx in range(len(df_train))]\n",
    "df_valid[\"input\"] = [preprocess_news(df_valid, idx, M, N) for idx in range(len(df_valid))]\n",
    "df_test[\"input\"] = [preprocess_news(df_test, idx, M, N) for idx in range(len(df_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1833c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451b1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[10:]\n",
    "df_valid = df_valid[10:]\n",
    "df_test = df_test[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c5c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Hybrid Attention Sequential Stock Model\n",
    "class NewsGRU(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size, num_layers):\n",
    "        super(NewsGRU, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size * 5, hidden_size, num_layers, batch_first=True)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape [batch_size, num_days, max_length]\n",
    "        batch_size, num_days, max_length = x.size()\n",
    "\n",
    "        # Reshape the input to feed it to BERT\n",
    "        x = x.view(-1, max_length)\n",
    "\n",
    "        # Get BERT embeddings\n",
    "        embeddings = self.bert(x).last_hidden_state  # shape: [batch_size * num_days, max_length, hidden_size]\n",
    "\n",
    "        # Reshape the embeddings to feed them to LSTM\n",
    "        embeddings = embeddings.view(batch_size, num_days, max_length, self.bert.config.hidden_size).permute(0, 2, 1, 3).contiguous()\n",
    "        embeddings = embeddings.view(batch_size, max_length, -1)  # shape: [batch_size, max_length, num_days * hidden_size]\n",
    "        \n",
    "\n",
    "        # Get the LSTM output\n",
    "        lstm_output, _ = self.lstm(embeddings)  # shape: [batch_size, max_length, num_days, hidden_size]\n",
    "\n",
    "        # Pool the LSTM output\n",
    "        lstm_pooled = self.pool(lstm_output.permute(0, 2, 1)).squeeze(2)  # shape: [batch_size, hidden_size]\n",
    "\n",
    "        return lstm_pooled\n",
    "\n",
    "class NewsAttention(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(NewsAttention, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape [batch_size, num_days, max_length]\n",
    "        batch_size, num_days, max_length = x.size()\n",
    "\n",
    "        # Reshape the input to feed it to BERT\n",
    "        x = x.view(-1, max_length)\n",
    "\n",
    "        # Get BERT embeddings and attention weights\n",
    "        outputs = self.bert(x, output_attentions=True)\n",
    "        \n",
    "        embeddings = outputs.last_hidden_state  # shape: [batch_size * num_days, max_length, hidden_size]\n",
    "        attentions = torch.cat(outputs.attentions, dim=1)  # shape: [batch_size * num_days, num_heads * num_layers, max_length, max_length]\n",
    "\n",
    "        # Calculate the attention-weighted embeddings\n",
    "        attention_weights = attentions.mean(dim=1)  # shape: [batch_size * num_days, max_length, max_length]\n",
    "        attention_embedding = torch.bmm(attention_weights, embeddings)  # shape: [batch_size * num_days, max_length, hidden_size]\n",
    "\n",
    "        # Reshape the embeddings to the original shape\n",
    "        attention_embedding = attention_embedding.view(batch_size, num_days, max_length, -1)  # shape: [batch_size, num_days, max_length, hidden_size]\n",
    "\n",
    "        # Pool the attention embeddings\n",
    "        pooled_attention = attention_embedding.mean(dim=[1, 2])  # shape: [batch_size, hidden_size]\n",
    "\n",
    "        return pooled_attention\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = self.fc(x)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context_vector = torch.sum(attention_weights * x, dim=1)\n",
    "        return context_vector\n",
    "    \n",
    "class HASSModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout):\n",
    "        super(HASSModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, gru_embedding, attention_embedding):\n",
    "        fused_embedding = torch.cat((gru_embedding, attention_embedding), dim=-1)\n",
    "        fused_embedding = self.dropout(fused_embedding)\n",
    "        logits = self.fc(fused_embedding)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb86276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "M = 5\n",
    "N = 7  # Number of past days to consider\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.3\n",
    "pretrained_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Initialize models\n",
    "gru_model = NewsGRU(pretrained_model_name, hidden_size=256, num_layers=1)\n",
    "attention_model = NewsAttention(pretrained_model_name)\n",
    "HASS = HASSModel(1024, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369b0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data_m, input_data_n = self.data.iloc[idx][\"input\"]\n",
    "        label = self.data.iloc[idx][\"Label\"]\n",
    "\n",
    "        input_data_m = torch.cat(input_data_m, dim=0)\n",
    "        input_data_n = torch.cat(input_data_n, dim=0)\n",
    "\n",
    "        return input_data_m, input_data_n, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23690512",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataset = NewsDataset(df_train)\n",
    "test_dataset = NewsDataset(df_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0f8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d2048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:12<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.7022, Train Accuracy: 0.5085, Test Loss: 0.6889, Test Accuracy: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.6954, Train Accuracy: 0.5130, Test Loss: 0.6984, Test Accuracy: 0.4550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.6922, Train Accuracy: 0.5300, Test Loss: 0.6870, Test Accuracy: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.6888, Train Accuracy: 0.5541, Test Loss: 0.6882, Test Accuracy: 0.5450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.6096, Train Accuracy: 0.6705, Test Loss: 0.7087, Test Accuracy: 0.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.3514, Train Accuracy: 0.8640, Test Loss: 0.8191, Test Accuracy: 0.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.1628, Train Accuracy: 0.9456, Test Loss: 1.0510, Test Accuracy: 0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0793, Train Accuracy: 0.9772, Test Loss: 1.1817, Test Accuracy: 0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0492, Train Accuracy: 0.9886, Test Loss: 1.2347, Test Accuracy: 0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 396/396 [03:13<00:00,  2.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [00:07<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0367, Train Accuracy: 0.9899, Test Loss: 1.3394, Test Accuracy: 0.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(gru_model.parameters()) + list(attention_model.parameters()) + list(HASS.parameters()), lr=1e-5)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gru_model.to(device)\n",
    "attention_model.to(device)\n",
    "HASS.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    gru_model.train()\n",
    "    attention_model.train()\n",
    "    HASS.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch_data_m, batch_data_n, batch_labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_data_m = batch_data_m.to(device)\n",
    "        batch_data_n = batch_data_n.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        gru_embedding = gru_model(batch_data_m)\n",
    "        attention_embedding = attention_model(batch_data_n)\n",
    "        logits = HASS(gru_embedding, attention_embedding)\n",
    "\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct_predictions += (predicted == batch_labels).sum().item()\n",
    "        total_predictions += batch_labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Evaluation\n",
    "    gru_model.eval()\n",
    "    attention_model.eval()\n",
    "    HASS.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data_m, batch_data_n, batch_labels in tqdm(test_loader):\n",
    "            batch_data_m = batch_data_m.to(device)\n",
    "            batch_data_n = batch_data_n.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            gru_embedding = gru_model(batch_data_m)\n",
    "            attention_embedding = attention_model(batch_data_n)\n",
    "            logits = HASS(gru_embedding, attention_embedding)\n",
    "\n",
    "            loss = criterion(logits, batch_labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct_predictions += (predicted == batch_labels).sum().item()\n",
    "            total_predictions += batch_labels.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c78cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1806f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11",
   "language": "python",
   "name": "cuda11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
